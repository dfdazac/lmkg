from pprint import pprint
from typing import Any, Optional

import torch
from huggingface_hub import InferenceClient
from transformers import PreTrainedModel, PreTrainedTokenizer

from .tools import AnswerStoreTool, GraphDBTool
from .utils import build_task_input, get_chat_template, run_if_callable


class LMKGAgent:
    """
    An agent designed to interact with a pre-trained language model and a
    knowledge graph database (GraphDB). It facilitates the generation of
    text-based responses using a language model and integrates with tools to
    process and retrieve information from a knowledge graph. It supports both
    local model inference and remote inference through an external endpoint.

    Args:
        functions: A list of function names that use with the graph database
        model: The pre-trained model to be used for local text generation.
            If None, external inference will be used.
        tokenizer: The tokenizer used for processing input and output to/from
            the model.
        chat_template: A string that specifies the chat format template.
        inference_endpoint: The URL of the inference endpoint for remote
            generation. If None, the local model will be used instead.
        graphdb_endpoint: The URL endpoint for accessing the graph database.
    """
    def __init__(self,
                 functions: list[str],
                 model: Optional[PreTrainedModel],
                 tokenizer: PreTrainedTokenizer,
                 chat_template: str,
                 inference_endpoint: Optional[str],
                 graphdb_endpoint: str):
        if not model and not inference_endpoint:
            raise ValueError("Either model or inference_endpoint must be "
                             "specified.")
        self.model = model
        self.tokenizer = tokenizer
        self.inference_client = None
        if inference_endpoint:
            self.inference_client = InferenceClient(inference_endpoint)
        self.graphdb_endpoint = graphdb_endpoint

        self.chat_template = get_chat_template(chat_template)
        self.graphdb = GraphDBTool(functions, graphdb_endpoint)

        self.messages = []

    def _append_message(self, role: str, content: Any):
        """
        Appends a message to the conversation history with a given role and
        content.

        Args:
            role: The role of the message sender.
            content: The content of the message, which can be a string or any
                object depending on the role.
        """
        self.messages.append({
            "role": role,
            "content": content
        })

    def run(self,
            task: str,
            task_kwargs: dict[str, str],
            max_responses: int,
            gen_config) -> Optional[str]:
        """
        Executes the agent's main task loop. It generates a response based on
        the task and its arguments, iterating over the conversation until a
        final answer is found.

        Args:
            task: The name or description of the task the agent should perform.
            task_kwargs: A dictionary of keyword arguments to further specify
                the task parameters.
            max_responses: The maximum number of responses allowed before
                stopping generation.
            gen_config: A configuration object specifying generation parameters
                such as `do_sample`, `temperature`, `top_k`, and `top_p`.

        Returns:
            The final answer generated by the agent after iterating through the
                conversation and tool results.
        """
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        answer_store = AnswerStoreTool()

        # Build system prompt, tool definition prompt, and task description
        task_prompt = build_task_input(task, task_kwargs)
        self.messages = []
        self._append_message("user", task_prompt)

        done = False
        num_responses = 0
        printed_system = False
        # Generation loop
        while not done:
            inputs = self.tokenizer.apply_chat_template(
                self.messages,
                tools=self.graphdb.tools_json + answer_store.tools_json,
                chat_template=self.chat_template,
                tokenize=self.inference_client is None,
                add_generation_prompt=True,
                return_dict=self.inference_client is None,
                return_tensors="pt"
            )

            if self.inference_client:
                # Generating from TGI endpoint
                outputs = self.inference_client.text_generation(
                    inputs,
                    return_full_text=False,
                    max_new_tokens=512,
                    do_sample=gen_config.do_sample,
                    temperature=gen_config.temperature,
                    top_k=gen_config.top_k,
                    top_p=gen_config.top_p,
                    details=False
                )
            else:
                # Generating from local model
                inputs = inputs.to(device)
                input_length = inputs['input_ids'].shape[-1]
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=2048,
                    pad_token_id=self.tokenizer.eos_token_id,
                    generation_config=gen_config)

                outputs = self.tokenizer.decode(outputs[0][input_length:])

            num_responses += 1
            if not printed_system:
                if not self.inference_client:
                    inputs = self.tokenizer.decode(inputs['input_ids'][0])
                print(inputs)
                printed_system = True

            self._append_message("assistant", outputs)
            print(outputs)

            tool_result, match_info = run_if_callable(
                outputs,
                tools=[self.graphdb, answer_store]
            )
            if tool_result:
                if match_info:
                    pprint(match_info)
                    self._append_message("ipython",
                                         {"output": match_info})
                pprint(tool_result)
                self._append_message("ipython",
                                     {"output": tool_result})
            elif answer_store.answer is None:
                self._append_message("user",
                                     "You forgot to submit the answer!")
            else:
                done = True

            if num_responses == max_responses:
                # TODO: add a warning
                break

        return answer_store.answer
